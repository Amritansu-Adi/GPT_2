{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hVZEYue8mt8S"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import inspect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "t1TkuHEsvFiv"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # # attention (materialise the large (T, T) matrix for all the queries and keys)\n",
        "        # att = q  @ k.transpose(-2, -1) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(-'inf'))\n",
        "        # att = F.softmax(att, dim = -1)\n",
        "        # y = att @ v #( B, nh, T, T) * (B, nh, T, hs) -> (B, bh, T, hs)\n",
        "\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024 # max sequence length\n",
        "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
        "    n_layer: int = 12 # number of layers\n",
        "    n_head: int = 12 # number of heads\n",
        "    n_embd: int = 768 # embedding dimension\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weights sharing scheme\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        self.apply(self.__init_weights)\n",
        "\n",
        "    def __init_weights(self, module):\n",
        "\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
        "        # start with all of the candidate parameters (that require grad)\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        # if master_process:\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and \"cuda\" in device\n",
        "        # if master_process:\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "        return optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "GOhEu6GEM2GH",
        "outputId": "3bd4242d-b501-4281-a202-4d5d1736f005"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
            "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\ml-cuda\\lib\\site-packages (from transformers) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\envs\\ml-cuda\\lib\\site-packages (from transformers) (2.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\envs\\ml-cuda\\lib\\site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\envs\\ml-cuda\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Collecting regex!=2019.12.17 (from transformers)\n",
            "  Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
            "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\envs\\ml-cuda\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.2-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\envs\\ml-cuda\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\envs\\ml-cuda\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\envs\\ml-cuda\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\ml-cuda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\ml-cuda\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\ml-cuda\\lib\\site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\ml-cuda\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\ml-cuda\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
            "Downloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
            "   ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
            "   --------- ------------------------------ 2.6/10.8 MB 11.6 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 5.2/10.8 MB 12.3 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 7.6/10.8 MB 12.1 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 10.0/10.8 MB 11.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 10.8/10.8 MB 11.7 MB/s eta 0:00:00\n",
            "Downloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
            "Downloading tokenizers-0.21.2-cp39-abi3-win_amd64.whl (2.5 MB)\n",
            "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
            "   ------------------------------------- -- 2.4/2.5 MB 12.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.5/2.5 MB 10.3 MB/s eta 0:00:00\n",
            "Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
            "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
            "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
            "\n",
            "   -------- ------------------------------- 1/5 [regex]\n",
            "   -------- ------------------------------- 1/5 [regex]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
            "   ------------------------ --------------- 3/5 [tokenizers]\n",
            "   ------------------------ --------------- 3/5 [tokenizers]\n",
            "   ------------------------ --------------- 3/5 [tokenizers]\n",
            "   ------------------------ --------------- 3/5 [tokenizers]\n",
            "   ------------------------ --------------- 3/5 [tokenizers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   -------------------------------- ------- 4/5 [transformers]\n",
            "   ---------------------------------------- 5/5 [transformers]\n",
            "\n",
            "Successfully installed huggingface-hub-0.33.4 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.2 transformers-4.53.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The scripts huggingface-cli.exe and tiny-agents.exe are installed in 'C:\\Users\\Amritansu Aditya\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The scripts transformers-cli.exe and transformers.exe are installed in 'C:\\Users\\Amritansu Aditya\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading weights from pretrained gpt: gpt2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52ec17b6d9514d46a5efd13b4848cf4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Amritansu Aditya\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Amritansu Aditya\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "428cff6611e64ab28bb4d38b9c0495c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "266d9b1e79dc4250b2dadc077383303e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (gelu): GELU(approximate='tanh')\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_return_sequences = 5\n",
        "max_length = 30\n",
        "!pip install transformers\n",
        "model = GPT.from_pretrained('gpt2')\n",
        "model.eval()\n",
        "model.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nod_9brUOAEN"
      },
      "outputs": [],
      "source": [
        "#prefix tokens\n",
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "tokens = enc.encode(\"Hello, my name is\")\n",
        "tokens = torch.tensor(tokens, dtype = torch.long) #(0, )\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) #(5, 8)\n",
        "x = tokens.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDdjXViw0Leq"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "while x.size(1) < max_length:\n",
        "    logits, _ = model(x)\n",
        "    logits = logits[:, -1, :]\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    topk_probs, topk_indices = torch.topk(probs, 50, dim = -1)\n",
        "    ix = torch.multinomial(topk_probs, num_samples=1)\n",
        "    xcol = torch.gather(topk_indices, -1, ix)\n",
        "    x = torch.cat((x, xcol), dim = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UzZuv4rNWBH"
      },
      "outputs": [],
      "source": [
        "for i in range(num_return_sequences):\n",
        "    # print(enc.decode(x[i].tolist()))\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6N9QeUqANGj",
        "outputId": "cab6ec17-c8f2-444c-cf45-1c104441cd16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-07-07 11:05:11--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.2’\n",
            "\n",
            "\rinput.txt.2           0%[                    ]       0  --.-KB/s               \rinput.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-07-07 11:05:11 (29.4 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ],
      "source": [
        "# tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "data = text[:1000] # first 1,000 characters\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cop8rhHNI-a_",
        "outputId": "f0857593-ef4c-49c2-db8f-21cfa39008a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13]\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "tokens = enc.encode(data)\n",
        "print(tokens[:24])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U80ZH3iyJBMT",
        "outputId": "a49bdb74-f0cf-4f70-c17e-94bb8a860b83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 5962, 22307,    25,   198,  8421,   356],\n",
              "         [ 5120,   597,  2252,    11,  3285,   502],\n",
              "         [ 2740,    13,   198,   198,  3237,    25],\n",
              "         [  198,  5248,   461,    11,  2740,    13]]),\n",
              " tensor([[22307,    25,   198,  8421,   356,  5120],\n",
              "         [  597,  2252,    11,  3285,   502,  2740],\n",
              "         [   13,   198,   198,  3237,    25,   198],\n",
              "         [ 5248,   461,    11,  2740,    13,   198]]))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "buf = torch.tensor(tokens[:24 + 1])\n",
        "x = buf[: -1].view(4, 6)\n",
        "y = buf[1:].view(4, 6)\n",
        "x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdEadBCYMRl8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "device = 'cuda'\n",
        "torch.manual_seed(1337)\n",
        "torch.cuda.manual_seed(1337)\n",
        "#prefix tokens\n",
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "# tokens = enc.encode(\"Hello, my name is\")\n",
        "# with open('input.txt', 'r') as f:\n",
        "#   text = f.read()\n",
        "# text = text[:1000]\n",
        "\n",
        "# tokens = torch.tensor(tokens, dtype = torch.long) #(0, )\n",
        "# tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) #(5, 8)\n",
        "# x = tokens.to('cuda')\n",
        "\n",
        "# tokens = enc.encode(text)\n",
        "# B, T = 4, 32\n",
        "# buf = torch.tensor(tokens[: B * T + 1])\n",
        "# buf = buf.to(device)\n",
        "# x = buf[:-1].view(B, T)\n",
        "# y = buf[1:].view(B, T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dVwdHBTMXGb"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import numpy as np\n",
        "\n",
        "def load_tokens(filename):\n",
        "    npt = np.load(filename)\n",
        "    npt = npt.astype(np.int32) # added after video\n",
        "    ptt = torch.tensor(npt, dtype=torch.long)\n",
        "    return ptt\n",
        "\n",
        "\n",
        "class DataLoaderLite:\n",
        "  # def __init__(self, B, T, process_rank, num_processes):\n",
        "  #   self.B = B\n",
        "  #   self.T = T\n",
        "  #   self.process_rank = process_rank\n",
        "  #   self.num_processes = num_processes\n",
        "\n",
        "  #   #at init load tokens from disk and store themin memory\n",
        "  #   with open('input.txt', 'r') as f:\n",
        "  #     text = f.read()\n",
        "  #   enc = tiktoken.get_encoding('gpt2')\n",
        "  #   tokens = enc.encode(text)\n",
        "\n",
        "  #   self.tokens = torch.tensor(tokens)\n",
        "  #   print(f\"loaded {len(tokens)} tokens\")\n",
        "  #   print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
        "\n",
        "  #   #state\n",
        "  #   self.current_position = self.B * self.T * self.process_rank\n",
        "\n",
        "  # def next_batch(self):\n",
        "  #   B, T = self.B, self.T\n",
        "  #   buf = self.tokens[self.current_position : self.current_position + B * T + 1]\n",
        "  #   x = buf[:-1].view(B, T)\n",
        "  #   y = buf[1:].view(B, T)\n",
        "  #   #advance the position in tensor\n",
        "  #   self.current_position += B * T * self.num_processes\n",
        "\n",
        "  #   #if loading the next batch would be out of bound\n",
        "  #   if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
        "  #     self.current_position = self.B * self.T * self.process_rank\n",
        "\n",
        "    #   return x, y\n",
        "    def __init__(self, B, T, process_rank, num_processes, split):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "        self.process_rank = process_rank\n",
        "        self.num_processes = num_processes\n",
        "        assert split in {'train', 'val'}\n",
        "\n",
        "        # get the shard filenames\n",
        "        data_root = \"edu_fineweb10B\"\n",
        "        shards = os.listdir(data_root)\n",
        "        shards = [s for s in shards if split in s]\n",
        "        shards = sorted(shards)\n",
        "        shards = [os.path.join(data_root, s) for s in shards]\n",
        "        self.shards = shards\n",
        "        assert len(shards) > 0, f\"no shards found for split {split}\"\n",
        "        if master_process:\n",
        "            print(f\"found {len(shards)} shards for split {split}\")\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # state, init at shard zero\n",
        "        self.current_shard = 0\n",
        "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
        "        self.current_position = self.B * self.T * self.process_rank\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
        "        x = (buf[:-1]).view(B, T) # inputs\n",
        "        y = (buf[1:]).view(B, T) # targets\n",
        "        # advance the position in the tensor\n",
        "        self.current_position += B * T * self.num_processes\n",
        "        # if loading the next batch would be out of bounds, advance to next shard\n",
        "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
        "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
        "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
        "            self.current_position = B * T * self.process_rank\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmKPjPDeJwT5",
        "outputId": "fb0fcea5-9a80-448c-e407-19e55f60c80f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device: cuda\n",
            "total desired batch size: 524288\n",
            "=> calculated gradient accumulation steps: 8\n",
            "loaded 338025 tokens\n",
            "1 epoch = 5 batches\n",
            "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ],
      "source": [
        "# gradient accumulation\n",
        "\n",
        "# total_batch_size = 524288 # 2**19, ~0.5M, in number of tokens\n",
        "# B = 4 # micro batch size\n",
        "# T = 1024 # sequence length\n",
        "# assert total_batch_size % (B * T) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
        "# grad_accum_steps = total_batch_size // (B * T)\n",
        "# # if master_process:\n",
        "# print(f\"total desired batch size: {total_batch_size}\")\n",
        "# print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# simple launch:\n",
        "# python train_gpt2.py\n",
        "# DDP launch for e.g. 8 GPUs:\n",
        "# torchrun --standalone --nproc_per_node=8 train_gpt2.py\n",
        "\n",
        "# run the training loop\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.distributed as dist\n",
        "import os\n",
        "\n",
        "# set up DDP (distributed data parallel).\n",
        "# torchrun command sets the env variables RANK, LOCAL_RANK, and WORLD_SIZE\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "if ddp:\n",
        "    # use of DDP atm demands CUDA, we set the device appropriately according to rank\n",
        "    assert torch.cuda.is_available(), \"for now i think we need CUDA for DDP\"\n",
        "    init_process_group(backend='nccl')\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "else:\n",
        "    # vanilla, non-DDP run\n",
        "    ddp_rank = 0\n",
        "    ddp_local_rank = 0\n",
        "    ddp_world_size = 1\n",
        "    master_process = True\n",
        "    # attempt to autodetect device\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "    print(f\"using device: {device}\")\n",
        "\n",
        "# added after video, pytorch can be serious about it's device vs. device_type distinction\n",
        "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "total_batch_size = 524288 # 2**19, ~0.5M, in number of tokens\n",
        "B = 64 # micro batch size\n",
        "T = 1024 # sequence length\n",
        "assert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
        "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
        "if master_process:\n",
        "    print(f\"total desired batch size: {total_batch_size}\")\n",
        "    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
        "\n",
        "\n",
        "# change precission for faster proccessing\n",
        "torch.set_float32_matmul_precision('medium')\n",
        "\n",
        "\n",
        "# create model\n",
        "# train_loader = DataLoaderLite(B, T)\n",
        "train_loader = DataLoaderLite(B = B, T = T, process_rank = ddp_rank, num_processes = ddp_world_size, split = 'train')\n",
        "\n",
        "# x, y = train_loader.next_batch()\n",
        "model = GPT(GPTConfig(vocab_size=50304))\n",
        "model.to(device)\n",
        "# logits, loss = model(x, y)\n",
        "# print(loss)\n",
        "model = torch.compile(model)\n",
        "if ddp:\n",
        "  model = DDP(model, device_ids = [ddp_local_rank])\n",
        "raw_model = model.module if ddp else model   # always contains the \"raw\" unwraped model\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 715\n",
        "max_steps = 19073 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it+1) / warmup_steps\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "# optimize!\n",
        "optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device)\n",
        "\n",
        "# optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device)\n",
        "# optimiser = torch.optim.AdamW(model.parameters(), lr = 3e-4, betas = (0.9, 0.95), eps = 1e-8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDIbcCcE5TJK"
      },
      "outputs": [],
      "source": [
        "import torch.distributed as dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "l7DzssvtZTiG",
        "outputId": "a5492d91-53dd-44c0-d103-391a74505e14"
      },
      "outputs": [
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.47 GiB is free. Process 114001 has 13.27 GiB memory in use. Of the allocated memory 9.09 GiB is allocated by PyTorch, and 4.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-17-758449335.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m# we have to scale the loss to account for gradient accumulation,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# because the gradients just add on each successive backward().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m                 \u001b[0;31m# Restore the dynamic layer stack depth if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2-1075069691.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# forward the blocks of the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;31m# forward the final layernorm and the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2-1075069691.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2-1075069691.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# y = att @ v #( B, nh, T, T) * (B, nh, T, hs) -> (B, bh, T, hs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# flash attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# re-assemble all head outputs side by side\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.47 GiB is free. Process 114001 has 13.27 GiB memory in use. Of the allocated memory 9.09 GiB is allocated by PyTorch, and 4.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "for step in range(max_steps):\n",
        "    t0 = time.time()\n",
        "\n",
        "    last_step = (step == max_steps - 1)\n",
        "\n",
        "    # once in a while evaluate our validation loss\n",
        "    if step % 250 == 0 or last_step:\n",
        "        model.eval()\n",
        "        val_loader.reset()\n",
        "        with torch.no_grad():\n",
        "            val_loss_accum = 0.0\n",
        "            val_loss_steps = 20\n",
        "            for _ in range(val_loss_steps):\n",
        "                x, y = val_loader.next_batch()\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "                    logits, loss = model(x, y)\n",
        "                loss = loss / val_loss_steps\n",
        "                val_loss_accum += loss.detach()\n",
        "        if ddp:\n",
        "            dist.all_reduce(val_loss_accum, op=dist.ReduceOp.AVG)\n",
        "        if master_process:\n",
        "            print(f\"validation loss: {val_loss_accum.item():.4f}\")\n",
        "            # with open(log_file, \"a\") as f:\n",
        "            #     f.write(f\"{step} val {val_loss_accum.item():.4f}\\n\")\n",
        "            # if step > 0 and (step % 5000 == 0 or last_step):\n",
        "            #     # optionally write model checkpoints\n",
        "            #     checkpoint_path = os.path.join(log_dir, f\"model_{step:05d}.pt\")\n",
        "            #     checkpoint = {\n",
        "            #         'model': raw_model.state_dict(),\n",
        "            #         'config': raw_model.config,\n",
        "            #         'step': step,\n",
        "            #         'val_loss': val_loss_accum.item()\n",
        "            #     }\n",
        "            #     # you might also want to add optimizer.state_dict() and\n",
        "            #     # rng seeds etc., if you wanted to more exactly resume training\n",
        "            #     torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "      # once in a while generate from the model (except step 0, which is noise)\n",
        "    if ((step > 0 and step % 100 == 0) or last_step) and (not use_compile):\n",
        "        model.eval()\n",
        "        num_return_sequences = 4\n",
        "        max_length = 32\n",
        "        tokens = enc.encode(\"Hello, I'm a language model,\")\n",
        "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "        tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "        xgen = tokens.to(device)\n",
        "        sample_rng = torch.Generator(device=device)\n",
        "        sample_rng.manual_seed(42 + ddp_rank)\n",
        "        while xgen.size(1) < max_length:\n",
        "            # forward the model to get the logits\n",
        "            with torch.no_grad():\n",
        "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "                    logits, loss = model(xgen) # (B, T, vocab_size)\n",
        "                # take the logits at the last position\n",
        "                logits = logits[:, -1, :] # (B, vocab_size)\n",
        "                # get the probabilities\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # do top-k sampling of 50 (huggingface pipeline default)\n",
        "                # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "                topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "                # select a token from the top-k probabilities\n",
        "                # note: multinomial does not demand the input to sum to 1\n",
        "                ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
        "                # gather the corresponding indices\n",
        "                xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "                # append to the sequence\n",
        "                xgen = torch.cat((xgen, xcol), dim=1)\n",
        "        # print the generated text\n",
        "        for i in range(num_return_sequences):\n",
        "            tokens = xgen[i, :max_length].tolist()\n",
        "            decoded = enc.decode(tokens)\n",
        "            print(f\"rank {ddp_rank} sample {i}: {decoded}\")\n",
        "\n",
        "    model.train()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss_accum = 0.0\n",
        "\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "          x, y = train_loader.next_batch()\n",
        "          x, y = x.to(device), y.to(device)\n",
        "          with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "              logits, loss = model(x, y)\n",
        "          # we have to scale the loss to account for gradient accumulation,\n",
        "          # because the gradients just add on each successive backward().\n",
        "          # addition of gradients corresponds to a SUM in the objective, but\n",
        "          # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n",
        "          loss = loss / grad_accum_steps\n",
        "          loss_accum += loss.detach()\n",
        "\n",
        "          #syncronise ddp losses only at las step\n",
        "          if ddp:\n",
        "            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n",
        "\n",
        "          loss.backward()\n",
        "\n",
        "    if ddp:\n",
        "      dist.all_reduce(loss_accum, op = dist.ReduceOp.AVG)\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # determine and set lr for this iteration\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "      param_group['lr'] = lr\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "    dt = (t1 - t0)*1000   #time in ms\n",
        "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n",
        "    tokens_per_sec = tokens_processed / dt\n",
        "\n",
        "    if master_process:\n",
        "      print(f\"step {step :4d} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2TRhc4JOWE7",
        "outputId": "466f5f94-df89-40be-ab55-a71e2b5cc549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Jul  7 11:12:28 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P0             28W /   70W |    8224MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiC30ZUsN4Yq"
      },
      "outputs": [],
      "source": [
        "#clear gpu\n",
        "device = 'cuda'\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puOtYjziPN13",
        "outputId": "db372d94-5d4c-42f4-df9f-c04cde7c77e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import IPython\n",
        "IPython.Application.instance().kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKWzK79lP9ZM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s_tkhyA-WBr"
      },
      "source": [
        "## FineWeb .py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "6LjTr4Qk-aQk",
        "outputId": "97464b98-8a5e-4f27-d1d0-f00da9847d33"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name '__file__' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-2769881107.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# create the cache the local directory if it doesn't exist yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mDATA_CACHE_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_CACHE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "FineWeb-Edu dataset (for srs pretraining)\n",
        "https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu\n",
        "Downloads and tokenizes the data and saves data shards to disk.\n",
        "Run simply as:\n",
        "$ python fineweb.py\n",
        "Will save shards to the local directory \"edu_fineweb10B\".\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import multiprocessing as mp\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "from datasets import load_dataset # pip install datasets\n",
        "from tqdm import tqdm # pip install tqdm\n",
        "\n",
        "# ------------------------------------------\n",
        "local_dir = \"edu_fineweb10B\"\n",
        "remote_name = \"sample-10BT\"\n",
        "shard_size = int(1e8) # 100M tokens per shard, total of 100 shards\n",
        "\n",
        "# create the cache the local directory if it doesn't exist yet\n",
        "DATA_CACHE_DIR = os.path.join(os.path.dirname(__file__), local_dir)\n",
        "os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# download the dataset\n",
        "fw = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=remote_name, split=\"train\")\n",
        "\n",
        "# init the tokenizer\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "eot = enc._special_tokens['<|endoftext|>'] # end of text token\n",
        "def tokenize(doc):\n",
        "    # tokenizes a single document and returns a numpy array of uint16 tokens\n",
        "    tokens = [eot] # the special <|endoftext|> token delimits all documents\n",
        "    tokens.extend(enc.encode_ordinary(doc[\"text\"]))\n",
        "    tokens_np = np.array(tokens)\n",
        "    assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
        "    tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
        "    return tokens_np_uint16\n",
        "\n",
        "def write_datafile(filename, tokens_np):\n",
        "    np.save(filename, tokens_np)\n",
        "\n",
        "# tokenize all documents and write output shards, each of shard_size tokens (last shard has remainder)\n",
        "nprocs = max(1, os.cpu_count()//2)\n",
        "with mp.Pool(nprocs) as pool:\n",
        "    shard_index = 0\n",
        "    # preallocate buffer to hold current shard\n",
        "    all_tokens_np = np.empty((shard_size,), dtype=np.uint16)\n",
        "    token_count = 0\n",
        "    progress_bar = None\n",
        "    for tokens in pool.imap(tokenize, fw, chunksize=16):\n",
        "\n",
        "        # is there enough space in the current shard for the new tokens?\n",
        "        if token_count + len(tokens) < shard_size:\n",
        "            # simply append tokens to current shard\n",
        "            all_tokens_np[token_count:token_count+len(tokens)] = tokens\n",
        "            token_count += len(tokens)\n",
        "            # update progress bar\n",
        "            if progress_bar is None:\n",
        "                progress_bar = tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n",
        "            progress_bar.update(len(tokens))\n",
        "        else:\n",
        "            # write the current shard and start a new one\n",
        "            split = \"val\" if shard_index == 0 else \"train\"\n",
        "            filename = os.path.join(DATA_CACHE_DIR, f\"edufineweb_{split}_{shard_index:06d}\")\n",
        "            # split the document into whatever fits in this shard; the remainder goes to next one\n",
        "            remainder = shard_size - token_count\n",
        "            progress_bar.update(remainder)\n",
        "            all_tokens_np[token_count:token_count+remainder] = tokens[:remainder]\n",
        "            write_datafile(filename, all_tokens_np)\n",
        "            shard_index += 1\n",
        "            progress_bar = None\n",
        "            # populate the next shard with the leftovers of the current doc\n",
        "            all_tokens_np[0:len(tokens)-remainder] = tokens[remainder:]\n",
        "            token_count = len(tokens)-remainder\n",
        "\n",
        "    # write any remaining tokens as the last shard\n",
        "    if token_count != 0:\n",
        "        split = \"val\" if shard_index == 0 else \"train\"\n",
        "        filename = os.path.join(DATA_CACHE_DIR, f\"edufineweb_{split}_{shard_index:06d}\")\n",
        "        write_datafile(filename, all_tokens_np[:token_count])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpFWIyLM-bLi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml-cuda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
